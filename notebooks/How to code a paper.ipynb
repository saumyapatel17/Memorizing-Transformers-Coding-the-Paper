{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome!\n",
        "\n",
        "### Purpose:\n",
        "Implement a research paper!\n",
        "\n",
        "[Memorizing Transformers](https://arxiv.org/pdf/2203.08913.pdf) from Google by Wu, Rabe, Hutchins, Szegedy (seh-guh-dee)\n",
        "- [Google implementation\n",
        "](https://github.com/google-research/meliad/tree/main/transformer)\n",
        "- [Pytorch implementation - lucidrains](https://github.com/lucidrains/memorizing-transformers-pytorch)\n",
        "\n",
        "\n",
        "### Goal:\n",
        "\n",
        "\n",
        "\"What I cannot create, I do not understand.\" - Richard Feynman\n",
        "\n",
        "- Teach you to implement this paper.\n",
        "- Give you a **process/framework** that enables you to implement new research papers and, more importantly, your own ideas\n",
        "- Lots of ML concepts along the way\n",
        "- PyTorch practice\n",
        "\n",
        "### Why learn to implement papers?\n",
        "- Put your own ideas into the world (research, tool, startup)\n",
        "- Relevant right now (2023): open source LLMs like LLaMA, quantized models, parameter efficient fine-tuning methods...You can do a lot of interesting things without needing to spend millions of dollars on GPUs (would not have said so just a year or two ago...)\n",
        "- Valuable (career) skill to have"
      ],
      "metadata": {
        "id": "vNc6Twz4igFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Why this video series?\n",
        "- There's nowhere else to go! Very few resources cover this specific skill despite it being so valuable (Karpathy videos are by far the best resource I've seen)\n",
        "- Implementing a research paper can be a huge pain, and there's no manual for this stuff: it's either sit next to someone who's done it before, your advisor/coworker, or DIY.\n",
        "- I learned how to do this just diving in by myself. I want to share some tips and stupid mistakes I made to help save you time and make your life easier\n",
        "- Lots of resources on: how to apply a model using a high-level library, how do deep learning models work, how to build a toy model..."
      ],
      "metadata": {
        "id": "ou873Bv-jG_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Format (Karpathy insipred!)\n",
        "- Build piece by piece and explain what we do along the way\n",
        "- To present the problems and dead ends we run into instead of just presenting the finished solution. So lots of fumbling and looking at documentation and testing things; **seeing the finished code doesn't teach you nearly as much as seeing the process**\n",
        "- **For you: pause and try to implement each piece before you watch me do it. MAKE IT CHALLENGING FOR YOURSELF**\n",
        "\n",
        "### Prerequisites\n",
        "- I will explain as we go, but this is a fairly advanced series\n",
        "- Familiar with Python / PyTorch\n",
        "- Familiar with deep learning (Transformers)\n",
        "- If you understood and enjoyed [Andrej Karpathy's YouTube lectures](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) then this is a great place to continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9tZrzPweAEl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framework for implementing research paper, step by step\n",
        "### High level plan\n",
        "\n",
        "1. Pick a paper\n",
        "2. Identify paper idea, how they will implement it, how they will test it\n",
        "3. Identify components of the paper\n",
        "4. Have a (rough) pseudocode understanding of how things work and fit together\n",
        "5. Build the individual components\n",
        "6. Assemble them into a model\n",
        "7. Run and test the model\n"
      ],
      "metadata": {
        "id": "0Iipe9AuqlsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Tips for picking a paper\n",
        "- Paper selection is important!\n",
        "- Pick something you're interested in: this will probably take a while!\n",
        "- You have to hunt for details: details are often missing from a paper: they are forgotten, or it's assumed you know, or they're somewhere in the code, or you'll have to look at reference papers to see the details. (A model that tries something new on top of the an old established architecture won't elaborate the details of that architecture - you'll have to go look them up.)\n",
        "- If possible **choose something that has an existing implementation** so that you can cross-reference your work and find hidden details.\n",
        "    - Obviously, if an implementation already exists there's little need to write a new one. However, right now, for the purposes of practicing and developing your skills, having something to work against is extremely helpful. This is your \"labeled\" data :)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Be aware of the limitations of reading the source code:\n",
        "- The code can contain lots of performance tricks that will obscure the main ideas.\n",
        "- Or it will be part of an existing library with a lot of weight, making it very hard to see/understand the core ideas. (e.g. try reading huggingface code)\n",
        "- Not guaranteed to have good / any comments or documentation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2GDLmTRnAEoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to read a paper effectively\n",
        "([Video presentation of Memorizing Transformers by author Yuhuai Wu](https://www.youtube.com/watch?v=5AoOpFFjW28&t=2973s))\n",
        "\n",
        "### Multiple passes approach\n",
        "Read the paper multiple times with increasing levels of detail. Aim for a very brief, high-level first pass. With each successive reading, fill in more details. This is easier and more efficient than going line by line, top to bottom, trying to digest every detail and equation. You can usually get the main point of the paper in a few minutes (abstract, diagrams, results). In later passes you can read more of the introduction, architecture details, experiments, related works, etc. and deepen your understanding.\n",
        "\n",
        "### Checklist questions to make sure you understand\n",
        "- What's the idea in a sentence or two?\n",
        "- What's the motivation as framed by the authors? What's the problem that they are solving?\n",
        "- How do they attempt to solve it?\n",
        "- What is the main contribution of the paper?\n",
        "- How do they measure success?\n",
        "- Were they successful?\n",
        "- Keep a running list of questions and knowledge gaps for yourself\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h4j9nH6o-p4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MEMORIZING TRANSFORMERS**\n",
        "\n",
        "- **What's the idea in a sentence or two?**\n",
        "\n",
        "Language models would benefit from a long-term memory mechanism. Approximate kNN lookup into non-differentiable memory of recent key-value pairs improves language modeling.\n",
        "\n",
        "- **What's the motivation as framed by the authors? What's the problem they are solving?**\n",
        "\n",
        "Attending to far-away tokens is important in many situations. However, transformer performance is limited by context window size: transformer is quadratic (O(n^2)) time and space complexity, so doubling the context window quadruples the compute requirements. This is why we cannot just naively make the context window bigger. There are many solutions that try to resolve this problem and create long-range attention.\n",
        "\n",
        "- **How do they attempt to solve it?**\n",
        "\n",
        "Store key-value pairs at one layer of the network. On future passes through the model, perform approximate kNN search into the stored key-value pairs by using the current query projection. Perform regular QKV attention at this layer, and also perform QKV attention a second time using the current Q and the stored/retrieved KV. Then combine these two QKVs and pass them into the next layer as usual.  \n",
        "\n",
        "- **What is the main contribution of the paper?**\n",
        "\n",
        "New approach: 1) we retrieve the exact historical key-value matrices rather than averaged or summarized versions, 2) gradients do not flow back into the kNN memory, making it fast and scalable.\n",
        "\n",
        "Improvement to language modeling: this approach allows for better performance with fewer parameters than vanilla transformers (on long-range datasets). This approach can also be integrated into existing architecture and even existing pre-trained models (via fine-tuning)\n",
        "\n",
        "- **How will they measure success?**\n",
        "\n",
        "Does this improve the performance on datasets designed to test long-range capability (long documents)? -> perplexity / performance on long range datasets.\n",
        "\n",
        "- **Were they successful?**\n",
        "\n",
        "Main result: a memorizing transformer with 1 billion parameters has the same perplexity (on arXiv-math dataset) as a vanilla transformer with 8 billion parameters.\n",
        "\n",
        "Other: increasing the memory size provided consistent benefits up to 262,000 tokens\n",
        "\n",
        "Other: a vanilla transformer that is fine-tuned with external memory performs as well as a memorizing transform that is pretrained from scratch - instead of needing to pretrain a memorizing transformer you can just fine-tune an existing vanilla one with memory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **What are the question / knowledge gaps for me?**\n",
        "\n",
        "Question: did they outperform other models on long document datasets? Is this approach better? No comparison to other long-range models\n",
        "\n",
        "Question: all metrics are perplexity: can we measure new capability that memory would unlock? E.g. a classification accuracy on a task that requires answering questions about a novel, very long document for which regular transformers would score very very low.\n",
        "\n",
        "Question: how does external memory compare to expanding the context window? In terms of accuracy and in terms of compute.\n",
        "\n",
        "Question: how does this compare to other models for \"regular\" length documents?"
      ],
      "metadata": {
        "id": "5wCteucX2Yab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Components to build\n",
        "- Vanilla decoder model\n",
        "- XL attention layer\n",
        "- KNN augmented layer\n",
        "- KNN memory (add/remove functionality)\n",
        "- T5 relative position embedding scheme\n",
        "-\n",
        "\n",
        "\n",
        "### Tips for building effectively\n",
        "- Try to build the simplest thing possible, then add in complexity / features\n",
        "- Make things work first, focus on performance last\n",
        "- How will you test it? Work out the metric and goal before you start anything else.\n",
        "- Make testing and benchmarking repeatable and fast so that you can quickly measure progress"
      ],
      "metadata": {
        "id": "q-4HunpQ2S1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Papers to code resources\n",
        "(Feel free to submit additional resources)\n",
        "\n",
        "### Main resources\n",
        "- fast.ai course\n",
        "- Andrej Karpathy YouTube series (highly recommended)\n",
        "- https://nn.labml.ai/ - simple, readable, implementations\n",
        "- https://github.com/lucidrains - hundreds of high quality PyTorch implementations (highly recommended)\n",
        "\n",
        "### Blogs / forums with advice\n",
        "- https://jsatml.blogspot.com/2014/10/beginner-advice-on-learning-to.html\n",
        "- https://blog.briankitano.com/llama-from-scratch/\n",
        "- https://www.reddit.com/r/MachineLearning/comments/2h94uj/comment/ckqrn1t/\n",
        "- https://machinelearningmastery.com/dont-start-with-open-source-code-when-implementing-machine-learning-algorithms/\n",
        "- https://www.reddit.com/r/MachineLearning/comments/ilqa9a/d_how_do_you_approach_implementing_research_papers/\n",
        "-https://www.reddit.com/r/deeplearning/comments/i86y6v/how_to_start_implementing_papers/\n",
        "- https://www.reddit.com/r/MachineLearning/comments/y0dk5c/d_recent_ml_papers_to_implement_from_scratch/\n",
        "- https://news.ycombinator.com/item?id=34503362"
      ],
      "metadata": {
        "id": "liyMJBGFAErP"
      }
    }
  ]
}