{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiAENBmxNjqi"
      },
      "source": [
        "# Adding XL Recurrence to Transformers\n",
        "\n",
        "This part of the code demonstrates how Transformer-XL (an advanced version of Transformers) introduces recurrence to improve memory retention over long sequences.\n",
        "The provided code defines two classes: XLAttention and KNN_XLAttention, which are enhancements to the standard Transformer architecture, adding mechanisms for improved long-range dependency handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh6ondJNNjqj"
      },
      "source": [
        "### Libraries and Setup\n",
        "\n",
        "- torch: This is the PyTorch library, used for building and training machine learning models, especially neural networks.\n",
        "- nn: PyTorch's sub-library for defining neural network layers.\n",
        "- F: Contains functional operations like activation functions (e.g., ReLU) and other tensor manipulations.\n",
        "- einops: A helpful library for tensor manipulation like reshaping and performing complex operations (rearranging dimensions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txknvVWpMXoS",
        "outputId": "10910cf0-64b3-4259-d9ac-1b26d1b8e53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "!pip install einops\n",
        "from einops import rearrange, repeat, pack, unpack, einsum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2e0YzLpNb9b"
      },
      "source": [
        "### Recurrence with Transformer-XL\n",
        "\n",
        "All technical details of Transformer-XL recurrence in [paper.](https://arxiv.org/pdf/1901.02860.pdf)\n",
        "\n",
        "\"We introduce the notion of recurrence into our deep self-attention network. In particular, instead of computing the hidden states from scratch for\n",
        "each new segment, we reuse the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment,\n",
        "which builds up a recurrent connection between the segments. As a result, modeling very longterm dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, passing information from the previous segment can also resolve\n",
        "the problem of context fragmentation.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPY8iNirNjqm"
      },
      "source": [
        "### The Core Concept of Transformer-XL\n",
        "The key idea of Transformer-XL is adding recurrence to the Transformer model. In basic Transformers, each input sequence is processed independently. But in Transformer-XL:\n",
        "- The hidden states (or memory) from previous segments of data are reused.\n",
        "- This allows the model to remember long-term dependencies and make use of past information (for example, in long texts or time-series data).\n",
        "\n",
        "This memory helps build a recurrent connection between segments of data. It is like a rolling window of context that the model uses for each new input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ga4j5u7OdfY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1st segment: compute current kv projections [kv_1] and perform attention\n",
        "# 2nd segment: concatenate old kv projections with current kv projections [kv1 + kv2] and perform attention\n",
        "# 3rd segment: concatenate old kv projections with current kv projections [kv2 + kv3] and perform attention\n",
        "# 4th segment: concatenate old kv projections with current kv projections [kv3 + kv4] and perform attention\n",
        "# ...\n",
        "\n",
        "# 1st segment:\n",
        "seg_one_kv = [seg_1_layer_1_kv,\n",
        "            seg_1_layer_2_kv,\n",
        "            seg_1_layer_3_kv,\n",
        "              ...]\n",
        "\n",
        "# 2nd segment:\n",
        "seg_two_kv = [concatenate(seg_1_layer_1_kv, seg_2_layer_1_kv),\n",
        "            concatenate(seg_1_layer_2_kv, seg_2_layer_2_kv),\n",
        "            concatenate(seg_1_layer_3_kv, seg_2_layer_3_kv),\n",
        "                ...]\n",
        "\n",
        "# 3rd segment:\n",
        "seg_three_kv = [concatenate(seg_2_layer_1_kv, seg_3_layer_1_kv),\n",
        "            concatenate(seg_2_layer_2_kv, seg_3_layer_2_kv),\n",
        "            concatenate(seg_2_layer_3_kv, seg_3_layer_3_kv),\n",
        "                ...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XJQOqS0Njqm"
      },
      "source": [
        "### Preparing the Inputs\n",
        "\n",
        "- batch_size: Number of sequences processed at once (16 sequences).\n",
        "- seq_len: Length of each sequence (512 tokens).\n",
        "- head_dimension: The size of each attention head (10).\n",
        "- number_heads: The number of attention heads (8).\n",
        "- embedding_dimension: Size of the input feature vectors (13)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ByJ-eE2Qg6ax"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "seq_len = 512\n",
        "head_dimension = 10\n",
        "number_heads = 8\n",
        "embedding_dimension = 13\n",
        "scaling_factor = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_MDH0USNjqn"
      },
      "source": [
        "This generates fake data (random values) to simulate input. It mimics a batch of sequences where each token has a feature vector of length 13."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoreKWfeg6cy",
        "outputId": "989fce6c-8db3-40e0-ca13-992451913efa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Create fake training batch\n",
        "input_data = torch.randn((batch_size, seq_len, embedding_dimension))\n",
        "input_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9DxC2kNNjqn"
      },
      "source": [
        "### Projection Matrices\n",
        "\n",
        "These linear layers transform the input embeddings into different spaces to form the queries, keys, and values used in the attention mechanism. In multi-head attention, the projections are repeated for each head (hence the multiplication by number_heads * head_dimension)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rc5jviltg6e4"
      },
      "outputs": [],
      "source": [
        "# Initialize projection matrices\n",
        "query_matrix = nn.Linear(embedding_dimension, number_heads * head_dimension)\n",
        "key_matrix = nn.Linear(embedding_dimension, number_heads * head_dimension)\n",
        "value_matrix = nn.Linear(embedding_dimension, number_heads * head_dimension)\n",
        "output_matrix = nn.Linear(number_heads * head_dimension, embedding_dimension)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWZYDXVuNjqo"
      },
      "source": [
        "### Creating Keys, Values, and Queries\n",
        "\n",
        "These are components of the self-attention mechanism. Each input is transformed into these three vectors. They help the model decide how much focus each token should have on every other token in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqGRnliGg6gv",
        "outputId": "e0a98dcc-525c-4b15-8bfc-498b30c2f6dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Create KQV matrices with input data\n",
        "queries = query_matrix(input_data)\n",
        "keys = key_matrix(input_data)\n",
        "values = value_matrix(input_data)\n",
        "values.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w1hrfXtNjqo"
      },
      "source": [
        "### Cached Memory (Recurrent Connection)\n",
        "\n",
        "we create fake cached memory (xl_memory). This represents the past memory (previous segments' keys and values). The model will combine this with the current sequence's keys and values to maintain context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSZmlnWfg6io",
        "outputId": "f2567cba-47dc-41a6-f36c-fba172de2800"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 2, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Create a fake cached XL recurrence\n",
        "xl_memory = torch.randn(batch_size, seq_len,2,number_heads*head_dimension)\n",
        "xl_memory.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZwXKmTgNjqo"
      },
      "source": [
        "### Merging Old and New Keys and Values\n",
        "\n",
        "- xl_keys and xl_values: These come from the old segment (i.e., the memory from previous steps).\n",
        "- torch.cat(): This operation concatenates the keys and values from the new and old sequences, so the model can use both old and new information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnemPIXig6kg",
        "outputId": "d2ba61b5-0101-44ea-ad57-44bd1b562155"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "xl_keys, xl_values = xl_memory.unbind(dim=-2) # the function unbind() is used to separate a tensor xl_memory along a specified dimension, in this case, dim=-2, which refers to the second-to-last dimension.\n",
        "xl_keys.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6XKexY2g6mg",
        "outputId": "99f6ca69-c12b-47b6-ddee-aaaf6a33f755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 1024, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "keys = torch.cat((xl_keys, keys), dim=-2)\n",
        "values = torch.cat((xl_values, values), dim=-2)\n",
        "values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzRuzonlg6oo",
        "outputId": "d0fc8070-e9a3-4b4b-d694-bec462e6363d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "queries.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjOOVQM-Njqp"
      },
      "source": [
        "### Attention Mechanism (QK)\n",
        "\n",
        "- rearrange: This is used to reshape the tensors. It’s important for ensuring the queries, keys, and values are in the correct format for the attention mechanism.\n",
        "- einsum: This is shorthand for a more complex matrix multiplication. It calculates the dot product between the queries and keys, which is used to determine the attention scores (how much focus each token gets on another)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzZU6ut6g6qn",
        "outputId": "c0a5023a-e3ab-4667-d8b8-9cca8af86669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "queries: torch.Size([16, 8, 512, 10])\n",
            "keys: torch.Size([16, 8, 1024, 10])\n",
            "qk: torch.Size([16, 8, 512, 1024])\n"
          ]
        }
      ],
      "source": [
        "queries = rearrange(queries, 'b t (h d) -> b h t d', h = number_heads)\n",
        "keys    = rearrange(keys, 'b t (h d) -> b h t d', h = number_heads)\n",
        "qk      = einsum(queries, keys, 'b h i d, b h j d -> b h i j')\n",
        "\n",
        "print (\"queries:\", queries.shape)\n",
        "print (\"keys:\", keys.shape)\n",
        "print (\"qk:\", qk.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a6H1cAh-g6sv"
      },
      "outputs": [],
      "source": [
        "# Regular Self Attention QK (4,4)\n",
        "#\n",
        "# [    1., -1000., -1000., -1000.]\n",
        "# [    1.,     1., -1000., -1000.]\n",
        "# [    1.,     1.,     1., -1000.]\n",
        "# [    1.,     1.,     1.,     1.]\n",
        "\n",
        "\n",
        "\n",
        "# Transformer XL Self Attention QK (4,8)\n",
        "#\n",
        "# [    1.,     1.,     1.,     1.,     1., -1000., -1000., -1000.]\n",
        "# [    1.,     1.,     1.,     1.,     1.,     1., -1000., -1000.]\n",
        "# [    1.,     1.,     1.,     1.,     1.,     1.,     1., -1000.]\n",
        "# [    1.,     1.,     1.,     1.,     1.,     1.,     1.,     1.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ-W0h8Eij2l",
        "outputId": "a1256e78-a510-42a4-b8ef-58b581082185"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "i, j = qk.shape[-2:]\n",
        "j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR9427ajNjqp"
      },
      "source": [
        "### Masking (To Prevent Attention to Future Tokens)\n",
        "\n",
        "In tasks like language modeling, we don't want the model to attend to future tokens. This mask ensures that attention is only given to the current and previous tokens (not future ones)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b80xU6kvij4v",
        "outputId": "e9962f10-a138-41f7-c3b2-675158c99351"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Create mask\n",
        "mask = torch.ones((i,j), dtype = torch.bool).triu(j-i+1)\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jijajQKij6p"
      },
      "outputs": [],
      "source": [
        "qk = qk.masked_fill(mask, float('-inf'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_bnRcL7Njqq"
      },
      "source": [
        "### Applying Softmax and Attention\n",
        "\n",
        "- Softmax: This step turns the attention scores into probabilities. The model then uses these probabilities to decide how much influence each token should have on the others.\n",
        "- Matrix Multiplication (@): After computing the attention weights, we multiply them by the values to get the attended values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0l7cClxhij86"
      },
      "outputs": [],
      "source": [
        "# Apply softmax\n",
        "qk = F.softmax(qk, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7sO0uUGij_D",
        "outputId": "a41aab79-ab19-42aa-ee5f-9f3b0b8120f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0000, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "qk[0][0][0].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94OBE-5BikAz",
        "outputId": "bbf2d0c4-1ad6-4086-f2cb-21840b99ac1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qk: torch.Size([16, 8, 512, 1024])\n",
            "values: torch.Size([16, 8, 1024, 10])\n"
          ]
        }
      ],
      "source": [
        "# Separate values tensor into heads for multi-head attention and move dimensions for @ with qk\n",
        "values = rearrange(values, 'b t (h d) -> b h t d', h=number_heads)\n",
        "print (\"qk:\", qk.shape)\n",
        "print (\"values:\", values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHafWz3cg6ur",
        "outputId": "f0483694-0c24-4308-944c-0c3ebfa1dbeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 8, 512, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "qkv = qk@values\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8QK6_KtNjqr"
      },
      "source": [
        "- Rearranging the output: After applying attention, we use rearrange to bring the output back into the original shape.\n",
        "- output_matrix: The result of the attention mechanism is passed through a linear layer to bring it back to the original embedding size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wh5DBokj4Y1",
        "outputId": "dd0bbcc2-b274-4a34-cde8-eae33936f9ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 80])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Reassemble all heads\n",
        "qkv = rearrange(qkv, 'b h t d -> b t (h d)')\n",
        "qkv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5Dy27OAj4av",
        "outputId": "896ead1f-38d6-4212-86a0-f515a4f7fcb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=80, out_features=13, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "output_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQlpApHWkIYB",
        "outputId": "0f8f6dbe-3fb8-483f-bc5d-6ff1eb6a141e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "out = output_matrix(qkv)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX45ciB-Njqr"
      },
      "source": [
        "### **XLAttention Class**\n",
        "\n",
        "This class implements **Transformer-XL**-like behavior, with a focus on introducing **recurrence** via the `xl_memory` argument, which allows the model to use memory from previous sequences (past attention states) to improve long-range dependencies.\n",
        "\n",
        "- **Query, Key, Value Matrices**: The class first computes the **queries**, **keys**, and **values** for self-attention. These are generated using linear layers that project the input embedding into a space suited for multi-head attention.\n",
        "  \n",
        "- **XL Memory (Recurrent Memory)**: If `xl_memory` is provided, it prepends the **old memory** (previous keys and values) to the new keys and values, effectively allowing the model to \"remember\" past sequences. This memory is passed as part of the attention mechanism to maintain long-term context.\n",
        "\n",
        "- **Self-Attention**: The attention scores are computed between the queries and keys, followed by a **masking** step to ensure the model doesn’t attend to future tokens. The attention scores are then passed through a **softmax** function, and the weighted sum of values is computed.\n",
        "\n",
        "- **Output**: The resulting attended values are passed through a final linear layer to output the predictions. Also, the new key-value pairs are returned for the next recurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aTsWhKozkIaF"
      },
      "outputs": [],
      "source": [
        "class XLAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dimension,\n",
        "        heads = 8,\n",
        "        head_dimension = 32,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = head_dimension ** -0.5\n",
        "\n",
        "        self.query_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.key_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.value_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.output_matrix = nn.Linear(heads * head_dimension, embedding_dimension)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x, # batch_size, sequence_length, embedding_dimension\n",
        "        xl_memory = None\n",
        "    ):\n",
        "        batch_size, sequence_length = x.shape[:2]\n",
        "        queries = self.query_matrix(x)\n",
        "        keys = self.key_matrix(x)\n",
        "        values = self.value_matrix(x)\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            k_xl, v_xl = xl_memory.unbind(dim=-2) # unstack\n",
        "            keys = torch.cat((k_xl, keys), dim = -2) # prepend XL memory\n",
        "            values = torch.cat((v_xl, values), dim = -2) # prepend XL memory\n",
        "            xl_sequence_length = k_xl.shape[1]\n",
        "\n",
        "        queries = rearrange(queries, 'b t (h d) -> b h t d', h = self.heads)\n",
        "        keys    = rearrange(keys, 'b t (h d) -> b h t d', h = self.heads)\n",
        "        qk      = einsum(queries, keys, 'b h i d, b h j d -> b h i j')\n",
        "\n",
        "        qk = qk * self.scale\n",
        "\n",
        "        ############\n",
        "        # TODO\n",
        "        # qk = relative_position_values + qk\n",
        "        ############\n",
        "\n",
        "        i, j = qk.shape[-2:]\n",
        "        mask = torch.ones((i,j), dtype = torch.bool).triu(j-i+1)\n",
        "        qk = qk.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        qk = F.softmax(qk, dim=-1)\n",
        "\n",
        "        values = rearrange(values, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        qkv = qk@values\n",
        "        qkv = rearrange(qkv, 'b h t d -> b t (h d)')\n",
        "\n",
        "        #### Return XL Memories\n",
        "\n",
        "        keys = rearrange(keys, 'b h t d -> b t (h d)', h = self.heads)\n",
        "        values = rearrange(values, 'b h t d -> b t (h d)', h=self.heads)\n",
        "        kv_memories = torch.stack((keys, values), dim=-2) # (batch, sequence_len, 2, dimension)\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            xl_memories, current_input = kv_memories[:, :-xl_sequence_length], kv_memories[:, -xl_sequence_length:]\n",
        "            kv_to_add_xl = current_input\n",
        "        else:\n",
        "            kv_to_add_xl = kv_memories\n",
        "\n",
        "\n",
        "        out = self.output_matrix(qkv)\n",
        "\n",
        "\n",
        "\n",
        "        return out, kv_to_add_xl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXQQfcHJNjqs"
      },
      "source": [
        "### **KNN_XLAttention Class**\n",
        "\n",
        "This class adds a **KNN (k-nearest neighbor)** retrieval mechanism to the XLAttention model. It combines local attention (traditional self-attention) with attention over a set of \"retrieved\" memories using KNN.\n",
        "\n",
        "- **KNN Memory Retrieval**: The model uses a KNN search to find the **top-k nearest memories** from a previously stored set of memories (`knn.search`). These are added to the attention computation. By retrieving similar memories, the model can leverage past experiences to enhance its attention process.\n",
        "\n",
        "- **Memory and Gate Bias**: A **gate bias** is applied to control the contribution of the retrieved memories vs the current query-key-value attention. This bias helps blend the two attention mechanisms (local and KNN-retrieved) effectively.\n",
        "\n",
        "- **Combined Attention**: The attention values from the local attention (`qkv`) and the KNN-retrieved attention (`mem_qkv`) are combined using the gate bias. This mixture of both types of attentions is then passed through the output matrix to produce the final result.\n",
        "\n",
        "- **Memory Update**: After performing attention, the new key-value pairs are stored in the KNN memory, so they can be used in subsequent computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BZlTJTLZkIcZ"
      },
      "outputs": [],
      "source": [
        "class KNN_XLAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dimension,\n",
        "        heads = 8,\n",
        "        head_dimension = 32,\n",
        "        topk_retrieved_memories = 3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = head_dimension ** -0.5\n",
        "\n",
        "        self.query_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.key_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.value_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.output_matrix = nn.Linear(heads * head_dimension, embedding_dimension)\n",
        "\n",
        "        self.gate_bias = nn.Parameter(torch.randn(self.heads, 1, 1))\n",
        "        self.topk_retrieved_memories = topk_retrieved_memories\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x, # batch_size, sequence_length, embedding_dimension\n",
        "        knn,\n",
        "        xl_memory = None\n",
        "    ):\n",
        "        batch_size, sequence_length = x.shape[:2]\n",
        "        queries = self.query_matrix(x)\n",
        "        keys = self.key_matrix(x)\n",
        "        values = self.value_matrix(x)\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            k_xl, v_xl = xl_memory.unbind(dim = -2) # unstack\n",
        "            keys = torch.cat((k_xl, keys), dim = -2) # prepend XL memory\n",
        "            values = torch.cat((v_xl, values), dim = -2) # prepend XL memory\n",
        "            xl_sequence_length = k_xl.shape[1]\n",
        "\n",
        "        ### LOCAL ATTENTION\n",
        "\n",
        "        queries = rearrange(queries, 'b t (h d) -> b h t d', h = self.heads)\n",
        "        keys    = rearrange(keys, 'b t (h d) -> b h t d', h = self.heads)\n",
        "        qk      = einsum(queries, keys, 'b h i d, b h j d -> b h i j')\n",
        "\n",
        "        qk = qk * self.scale\n",
        "\n",
        "        ############\n",
        "        # TODO\n",
        "        # qk = relative_position_values + qk\n",
        "        ############\n",
        "\n",
        "        i, j = qk.shape[-2:]\n",
        "        mask = torch.ones((i,j), dtype = torch.bool).triu(j-i+1)\n",
        "        qk = qk.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        qk = F.softmax(qk, dim=-1)\n",
        "\n",
        "        values = rearrange(values, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        qkv = qk@values\n",
        "        qkv = rearrange(qkv, 'b h t d -> b t (h d)')\n",
        "\n",
        "        ### KNN ATTENTION\n",
        "\n",
        "        # Convert queries to search form\n",
        "        queries = rearrange(queries, 'b h t d -> b t (h d)')\n",
        "        mem_kv = knn.search(queries, topk = self.topk_retrieved_memories) # returns b t k 2 d\n",
        "        mem_k, mem_v = mem_kv.unbind(dim = -2)\n",
        "        mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.heads)\n",
        "        mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.heads)\n",
        "\n",
        "        # Convert queries to attention form\n",
        "        queries = rearrange(queries, 'b t (h d) -> b h t d', h = self.heads)\n",
        "        mem_qk = einsum('b h t d, b h t k d -> b h t k', queries, mem_k)\n",
        "        mem_qk = mem_qk * self.scale\n",
        "\n",
        "        mem_qk = F.softmax(mem_qk, dim=-1)\n",
        "        mem_qk = self.dropout(mem_qk)\n",
        "        mem_qkv = einsum('b h t k, b h t k d -> b h t d', mem_qk, mem_v)\n",
        "\n",
        "        # Combined attentions\n",
        "\n",
        "        combined_qkv = mem_qkv * self.gate_bias + qkv * (1 - self.gate_bias)\n",
        "        combined_qkv = rearrange(combined_qkv, 'b h t d -> b t (h d)')\n",
        "        out = self.output_matrix(combined_qkv)\n",
        "\n",
        "        # New XL memories\n",
        "        keys = rearrange(keys, 'b h t d -> b t (h d)', h = self.heads)\n",
        "        values = rearrange(values, 'b h t d -> b t (h d)', h=self.heads)\n",
        "        kv_memories = torch.stack((keys, values), dim=-2) # (batch, sequence_len, 2, dimension)\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            # if we're on a middle/end segment of a document (there are previous XL memories)\n",
        "            xl_memories, current_kv = kv_memories[:, :-xl_sequence_length], kv_memories[:, -xl_sequence_length:]\n",
        "        else:\n",
        "            # if we're at the first segment\n",
        "            current_kv = kv_memories\n",
        "\n",
        "        knn.add(current_kv)\n",
        "\n",
        "        return out, current_kv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Terminology:\n",
        "- b: This represents the batch size.\n",
        "- t: This represents the time or sequence length (e.g., number of tokens).\n",
        "- k: This typically refers to the number of memory vectors or keys.\n",
        "- h: This refers to the number of attention heads (a hyperparameter in attention mechanisms like transformers).\n",
        "- d: This refers to the dimensionality of each attention head's output."
      ],
      "metadata": {
        "id": "EROHMcH_VsPi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ34vnfHkIel"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0YQ0Qd-Njqs"
      },
      "source": [
        "### Key Concepts\n",
        "\n",
        "- Transformer-XL: Introduces recurrence to the transformer architecture by reusing memory from previous segments.\n",
        "- Self-attention: Each token in a sequence learns to focus on other tokens based on similarity (query-key matching).\n",
        "- Multi-head attention: Splits the attention process into multiple \"heads,\" allowing the model to focus on different parts of the sequence at once.\n",
        "- Masked self-attention: Ensures that tokens only attend to previous ones, preventing the model from cheating in autoregressive tasks.\n",
        "\n",
        "### Key Features of These Classes:\n",
        "1. **Recurrent Memory**: By utilizing `xl_memory`, these classes maintain state across input segments, which is particularly useful for processing long sequences, such as documents or time-series data.\n",
        "   \n",
        "2. **KNN Integration**: In `KNN_XLAttention`, the KNN memory retrieval allows the model to effectively use past \"experiences\" (memories) that are similar to the current input, which can improve its performance on tasks like language modeling and question answering where context and past information are important.\n",
        "\n",
        "3. **Scalability**: Both models are designed to scale effectively with large sequences, by using both recurrent memory (Transformer-XL) and retrieval-based mechanisms (KNN).\n",
        "\n",
        "### Applications\n",
        "These modifications to the attention mechanism are useful in tasks like language modeling, document processing, or time-series forecasting where maintaining long-term dependencies or leveraging past experiences is crucial. Transformer-XL-like recurrence improves memory efficiency, while KNN provides a way to retrieve relevant past experiences dynamically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFi6QMNIj4dA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wZuD9COj4fV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}