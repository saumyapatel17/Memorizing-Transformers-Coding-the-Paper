{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oajWoYGbtCVi"
   },
   "source": [
    "# Processing Data to Keep GPUs Busy\n",
    "\n",
    "This part of the script focuses on preparing a dataset for efficient processing by the GPU. The data preparation steps ensure the GPUs remain fully utilized during training by dividing tasks into manageable chunks and keeping data pipelines optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz9LdFCttCVj"
   },
   "source": [
    "4 steps.\n",
    "1) get dataset\n",
    "2) store and load data\n",
    "3) process/tokenize data\n",
    "4) load a batch of data and send it to the GPU\n",
    "\n",
    "![Architecture](../Images/image-2.png)\n",
    "\n",
    "![Data processing pipeline](../Images/image-3.png)\n",
    "\n",
    "### The Point: Make Training Fast\n",
    "Your goal: time(data processing pipeline) <= time(GPU iteration)\n",
    "- The bottleneck is the slowest part of your training loop\n",
    "- Identify the bottleneck, remove it,repeat\n",
    "- The GPU should be your bottleneck (flops or bandwidth cost) in most cases because GPUs are the more expensive and limited resource. GPU optimization is its own topic :)\n",
    "- Therefore the CPU data processing pipeline should not be the bottleneck\n",
    "- Your goal is to load batch of data faster than the GPU can process it. If you can do this, then you don't need to optimize the data processing pipeline - optimizing it will have no effect on training speed\n",
    "- If your GPU is sitting idle waiting for a new batch of data then your data processing pipeline is a bottleneck and you should optimize\n",
    "⁃ Optimize the data processing pipeline by profiling, accurately identifying which step is the bottleneck, and applying optimizations specific to that part of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atr7Hl4CtCVk"
   },
   "source": [
    "### Libraries and Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiAxHKkTvhmD",
    "outputId": "82fb4669-00be-4d11-9d4b-919561667615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "!pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9l_s_GhtCVm"
   },
   "source": [
    "##### Purpose:\n",
    "- datasets is used to load a large dataset for text summarization.\n",
    "- The dataset is streamed and reduced to 3,500 examples to limit memory usage.\n",
    "\n",
    "##### Streaming\n",
    "Refers to the ability to load and process data from a dataset incrementally, rather than loading the entire dataset into memory at once. This is particularly useful when working with large datasets that cannot fit into memory. The streaming=True option in the datasets.load_dataset function provided by the Hugging Face Datasets library enables this behavior.\n",
    "\n",
    "##### How Streaming Works in Hugging Face Datasets:\n",
    "- On-Demand Loading: Data is loaded one sample or batch at a time, directly from the source (e.g., a remote server or file storage) without needing to download and store the entire dataset locally.\n",
    "- Efficient Resource Usage: It minimizes memory usage, as only the required data points are kept in memory during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vCRmD-TH7CAI"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"ccdv/arxiv-summarization\", split='train', streaming=True)\n",
    "raw_dataset = list(dataset.take(3500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgEbiSPVtCVm"
   },
   "source": [
    "### Chunking and Segmentation\n",
    "\n",
    "Why chunking?\n",
    "- Articles are too long to process directly.\n",
    "- Splitting into fixed-size chunks allows efficient batching and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul6t_RIEvF3S"
   },
   "source": [
    "Suppose we have a dataset of **scientific articles**, each with different lengths. For this example, assume:\n",
    "- The articles vary in length from 3000 to 10000 tokens.\n",
    "- **Segment length** is set to `512` tokens.\n",
    "- Each sequence (document) is split into **segments** of size 512.\n",
    "- **Chunk size** (total sequence length) is `5120` tokens (`10 segments × 512 tokens`).\n",
    "- A **batch** consists of `8` such sequences.\n",
    "\n",
    "#### **Documents (Dataset Level)**\n",
    "These are the raw articles in the dataset. For simplicity:\n",
    "- Document 1: 3500 tokens\n",
    "- Document 2: 7200 tokens\n",
    "- Document 3: 10240 tokens\n",
    "- ... (more documents)\n",
    "\n",
    "#### **Chunks**\n",
    "Each document is divided into chunks of **5120 tokens** (if long enough). For documents shorter than 5120 tokens, they might be discarded, padded, or processed differently depending on the task.\n",
    "\n",
    "- Document 2 (7200 tokens) → 1 full chunk of 5120 tokens + 1 leftover chunk of 2080 tokens (discarded or padded).\n",
    "- Document 3 (10240 tokens) → 2 full chunks of 5120 tokens.\n",
    "\n",
    "#### **Segments**\n",
    "Each **chunk** is further divided into smaller **segments** of `512 tokens` for processing. For example:\n",
    "- Document 3, Chunk 1 (5120 tokens) → split into 10 segments:\n",
    "  - Segment 1: Tokens 0–511\n",
    "  - Segment 2: Tokens 512–1023\n",
    "  - ...\n",
    "  - Segment 10: Tokens 4608–5119\n",
    "\n",
    "These **segments** allow processing piece-by-piece, reducing memory usage.\n",
    "\n",
    "#### **Batches**\n",
    "A **batch** consists of multiple sequences (chunks from different documents) that are processed together. With a batch size of `8`, a single batch might look like this:\n",
    "- Sequence 1: Chunk 1 of Document 2\n",
    "- Sequence 2: Chunk 2 of Document 3\n",
    "- Sequence 3: Chunk 1 of Document 4\n",
    "- ...\n",
    "- Sequence 8: Chunk 1 of Document 9\n",
    "\n",
    "Each batch is passed through the model in parallel to maximize training efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization\n",
    "\n",
    "| **Level**       | **Example**                                |\n",
    "|------------------|--------------------------------------------|\n",
    "| **Document**     | Entire raw text of a scientific article.   |\n",
    "| **Chunk**        | A slice of the document (5120 tokens).     |\n",
    "| **Segment**      | Smaller parts of a chunk (512 tokens each).|\n",
    "| **Batch**        | 8 sequences (chunks) processed together.   |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- **Segments** are subdivisions of a single chunk (smaller pieces of data for processing sequentially within one sequence).\n",
    "- **Chunks** are subdivisions of documents to create manageable sequences of fixed length (for memory constraints).\n",
    "- **Batches** combine multiple sequences (from chunks of different documents) for parallel processing.\n",
    "\n",
    "This hierarchy enables efficient memory management and parallelism during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_Lde8KnBm4O"
   },
   "outputs": [],
   "source": [
    "# BATCH SIZE: 4 (papers)\n",
    "# CHUNK SIZE: 5 (each paper broken into 5 chunks of n tokens each)\n",
    "\n",
    "\n",
    "#        forward pass 1 | FP 2    | FP 3    | FP 4    | FP 5    |\n",
    "#\n",
    "# paper 1:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 2:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 3:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 4:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "#\n",
    "#\n",
    "#\n",
    "#        forward pass 6 | FP 7    | FP 8    | FP 9    | FP 10   |\n",
    "#\n",
    "# paper 5:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 6:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 7:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n",
    "# paper 8:      chunk 1 | chunk 2 | chunk 3 | chunk 4 | chunk 5 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeBhyQicBm6L",
    "outputId": "1d2d7c5f-1912-4399-af7e-755be82d865e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = 10\n",
    "segment_length = 512\n",
    "chunk_size = segments * segment_length\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q6XrDeHtCVn"
   },
   "source": [
    "Articles are filtered to ensure each is long enough for chunking into 10 segments of 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGA3hRbbFQuD",
    "outputId": "6997b4c1-678e-41e0-f8d2-7521b159953d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of articles 3401\n"
     ]
    }
   ],
   "source": [
    "raw_articles = [x['article'] for x in raw_dataset]\n",
    "raw_articles = [x for x in raw_articles if len(x) > 5120]\n",
    "print (\"number of articles\", len(raw_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzM3GjNZFxuN",
    "outputId": "afca8931-de51-4cff-d6bd-3b22bf514f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character set length 70\n",
      "character set \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set(''.join([i for i in raw_articles]))\n",
    "print (\"character set length\", len(unique_chars))\n",
    "print (\"character set\", ''.join(sorted(unique_chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx3rzZBiwXjx"
   },
   "source": [
    "Converts the first raw article (a string) into an array of 8-bit integers (dtype=np.uint8), where each integer represents an ASCII code of a character in the article.\n",
    "\n",
    "- Extracts the first 512 tokens (characters) from the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GMPZR_DL36y",
    "outputId": "680b2f13-ad02-4ac8-862b-082c4ba3e26c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-06ffad3da219>:1: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  np.fromstring(raw_articles[0], dtype=np.uint8)[:512]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 97, 100, 100, 105, 116, 105, 118, 101,  32, 109, 111, 100, 101,\n",
       "       108, 115,  32,  64, 120,  99, 105, 116, 101,  32, 112, 114, 111,\n",
       "       118, 105, 100, 101,  32,  97, 110,  32, 105, 109, 112, 111, 114,\n",
       "       116,  97, 110, 116,  32, 102,  97, 109, 105, 108, 121,  32, 111,\n",
       "       102,  32, 109, 111, 100, 101, 108, 115,  32, 102, 111, 114,  32,\n",
       "       115, 101, 109, 105, 112,  97, 114,  97, 109, 101, 116, 114, 105,\n",
       "        99,  32, 114, 101, 103, 114, 101, 115, 115, 105, 111, 110,  32,\n",
       "       111, 114,  32,  99, 108,  97, 115, 115, 105, 102, 105,  99,  97,\n",
       "       116, 105, 111, 110,  32,  46,  32, 115, 111, 109, 101,  32, 114,\n",
       "       101,  97, 115, 111, 110, 115,  32, 102, 111, 114,  32, 116, 104,\n",
       "       101,  32, 115, 117,  99,  99, 101, 115, 115,  32, 111, 102,  32,\n",
       "        97, 100, 100, 105, 116, 105, 118, 101,  32, 109, 111, 100, 101,\n",
       "       108, 115,  32,  97, 114, 101,  32, 116, 104, 101, 105, 114,  32,\n",
       "       105, 110,  99, 114, 101,  97, 115, 101, 100,  32, 102, 108, 101,\n",
       "       120, 105,  98, 105, 108, 105, 116, 121,  32, 119, 104, 101, 110,\n",
       "        32,  99, 111, 109, 112,  97, 114, 101, 100,  32, 116, 111,  32,\n",
       "       108, 105, 110, 101,  97, 114,  32, 111, 114,  32, 103, 101, 110,\n",
       "       101, 114,  97, 108, 105, 122, 101, 100,  32, 108, 105, 110, 101,\n",
       "        97, 114,  32, 109, 111, 100, 101, 108, 115,  32,  97, 110, 100,\n",
       "        32, 116, 104, 101, 105, 114,  32, 105, 110,  99, 114, 101,  97,\n",
       "       115, 101, 100,  32, 105, 110, 116, 101, 114, 112, 114, 101, 116,\n",
       "        97,  98, 105, 108, 105, 116, 121,  32, 119, 104, 101, 110,  32,\n",
       "        99, 111, 109, 112,  97, 114, 101, 100,  32, 116, 111,  32, 102,\n",
       "       117, 108, 108, 121,  32, 110, 111, 110, 112,  97, 114,  97, 109,\n",
       "       101, 116, 114, 105,  99,  32, 109, 111, 100, 101, 108, 115,  32,\n",
       "        46,  32,  10,  32, 105, 116,  32, 105, 115,  32, 119, 101, 108,\n",
       "       108,  32,  45,  32, 107, 110, 111, 119, 110,  32, 116, 104,  97,\n",
       "       116,  32, 103, 111, 111, 100,  32, 101, 115, 116, 105, 109,  97,\n",
       "       116, 111, 114, 115,  32, 105, 110,  32,  97, 100, 100, 105, 116,\n",
       "       105, 118, 101,  32, 109, 111, 100, 101, 108, 115,  32,  97, 114,\n",
       "       101,  32, 105, 110,  32, 103, 101, 110, 101, 114,  97, 108,  32,\n",
       "       108, 101, 115, 115,  32, 112, 114, 111, 110, 101,  32, 116, 111,\n",
       "        32, 116, 104, 101,  32,  99, 117, 114, 115, 101,  32, 111, 102,\n",
       "        32, 104, 105, 103, 104,  32, 100, 105, 109, 101, 110, 115, 105,\n",
       "       111, 110,  97, 108, 105, 116, 121,  32, 116, 104,  97, 110,  32,\n",
       "       103, 111, 111, 100,  32, 101, 115, 116, 105, 109,  97, 116, 111,\n",
       "       114, 115,  32, 105, 110,  32, 102, 117, 108, 108, 121,  32, 110,\n",
       "       111, 110, 112,  97, 114,  97, 109, 101, 116, 114, 105,  99,  32,\n",
       "       109, 111, 100, 101, 108, 115,  32,  46,  32,  10,  32, 109,  97,\n",
       "       110, 121,  32, 101, 120], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fromstring(raw_articles[0], dtype=np.uint8)[:512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoLWur1AwdRI"
   },
   "source": [
    "The function decode_text takes a sequence of tokens (numerical values) and converts them into text by interpreting each token as an ASCII code. Here's what it does step by step:\n",
    "\n",
    "- Input: It accepts a sequence of integers (tokens).\n",
    "- Conversion to Characters: It uses the chr() function to convert each integer into its corresponding ASCII character.\n",
    "- Concatenation: The resulting characters are joined into a single string using ''.join()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "VBxR83jvL38c",
    "outputId": "2120d4c4-0feb-43d0-f864-863a86f92cca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-d550589d98d7>:4: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  decode_text(np.fromstring(raw_articles[0], dtype=np.uint8)[:512])\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models . \\n it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models . \\n many ex'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_text(tokens):\n",
    "    return ''.join([chr(i) for i in tokens])\n",
    "\n",
    "decode_text(np.fromstring(raw_articles[0], dtype=np.uint8)[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oG_c2kWbMpvj",
    "outputId": "9782bf1e-e389-4d0b-b118-c205ea91c418"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models . \\n it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models . \\n many ex'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_articles[0][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zbLG0zTNxSx",
    "outputId": "aec66f05-5cee-4f8b-bc3c-6bf1621ab73e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-6edb9ae0d54d>:1: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  converted = [np.fromstring(doc, dtype=np.uint8) for doc in raw_articles]\n"
     ]
    }
   ],
   "source": [
    "converted = [np.fromstring(doc, dtype=np.uint8) for doc in raw_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDwoK0XItCVp"
   },
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Articles are converted into a numerical format suitable for PyTorch tensors:\n",
    "- Encoding: Converts text to np.uint8 arrays for GPU processing.\n",
    "- Clipping: Ensures article lengths are multiples of the chunk size.\n",
    "- Chunking: Divides articles into smaller arrays of fixed size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKOxy7W2xHBp"
   },
   "source": [
    "Clipping ensures that documents can be split into equal-sized chunks (segments) of 5120 tokens. For example:\n",
    "\n",
    "If a document's original length is 5157 tokens, clipping removes 37 tokens, leaving 5120 tokens.\n",
    "This avoids issues with uneven chunking during further processing (e.g., when reshaping or dividing the data into smaller segments for batching or training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6r8d7cHcOFef"
   },
   "outputs": [],
   "source": [
    "def clip_article(doc, chunk_size):\n",
    "    remainder = len(doc) % chunk_size\n",
    "    return doc[:-remainder]\n",
    "\n",
    "clipped = [clip_article(doc, 5120) for doc in converted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rvKXRugmy0tZ"
   },
   "outputs": [],
   "source": [
    "clipped = [clip_article(doc, 5120) for doc in converted if len(doc) >= 5120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-g0PM6jkOXtJ",
    "outputId": "28c0a7ba-1bcf-4441-f36a-469dc3774cd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped[1].shape[0] / 5120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEEl9r1Ixv-f"
   },
   "source": [
    "This line of code processes the clipped articles by splitting each one into equally sized chunks (of size `chunk_size`) and then storing the resulting chunks in a new NumPy array.\n",
    "\n",
    "**Reshape Each Document**:\n",
    "   - For each document (`doc`) in `clipped`, `doc.reshape(-1, chunk_size)` splits the document into multiple chunks of size `chunk_size`.\n",
    "   - The `-1` in `.reshape(-1, chunk_size)` automatically determines the number of chunks required based on the document length.\n",
    "\n",
    "### Example\n",
    "Suppose:\n",
    "- `chunk_size = 5120`.\n",
    "- A document in `clipped` has 10240 tokens.\n",
    "\n",
    "The reshaping:\n",
    "- Splits the document into `10240 / 5120 = 2` chunks.\n",
    "- The resulting array for this document would have a shape of `(2, 5120)`.\n",
    "\n",
    "If `clipped` contains 3 such documents, `chunked` would be a 3D array:\n",
    "- Shape: `(3, 2, 5120)` (3 documents, each with 2 chunks, and each chunk having 5120 tokens).\n",
    "\n",
    "### Purpose\n",
    "The reshaping ensures that the data is ready for batch processing or sequential input into a machine learning model, where fixed-sized chunks are often required. It simplifies further operations, such as:\n",
    "- Combining chunks into batches for training.\n",
    "- Processing data sequentially in smaller segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "B_SMGUDUOXu3"
   },
   "outputs": [],
   "source": [
    "chunked = np.concatenate([doc.reshape(-1, chunk_size) for doc in clipped], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl9zsHsa0OYa"
   },
   "source": [
    "np.array() requires all elements to have the same shape to form a homogeneous array\n",
    "\n",
    "chunked will be a single 2D NumPy array where all reshaped documents are stacked together. The shape will be (total_segments, 5120) where total_segments is the sum of the rows across all reshaped documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZbN0INz2Api"
   },
   "source": [
    "- Tensors are an extension of arrays with additional features, commonly used in deep learning and PyTorch (but also found in TensorFlow and other ML libraries).\n",
    "- A tensor is essentially a multi-dimensional array that allows efficient operations on GPUs, which are crucial for training models.\n",
    "- Tensors can store not just numerical data, but also perform automatic differentiation, which is key for backpropagation in neural networks.\n",
    "- PyTorch tensors are similar to NumPy arrays, but have additional functionality, such as support for GPU computation and gradients for automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Bgh0takz8Sg",
    "outputId": "743aedde-5c44-4cbe-db4d-73174cf5b253"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20853, 5120])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts the list or array chunked into a PyTorch tensor with a data type of torch.long, which is commonly used for storing integer values.\n",
    "processed_data = torch.tensor(chunked, dtype=torch.long)\n",
    "processed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bGidXrytCVq"
   },
   "source": [
    "### DataLoader Setup\n",
    "\n",
    "- The dataset is split into 80% train, 10% validation, and 10% test subsets.\n",
    "- PyTorch DataLoader is used to create shuffled batches for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "kBJ2-wvzsubT"
   },
   "outputs": [],
   "source": [
    "eighty_split = int(processed_data.shape[0] * .8)\n",
    "ninety_split = int(processed_data.shape[0] * .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNHuQCFb23Hm"
   },
   "source": [
    "DataLoader is a convenient way to handle large datasets efficiently by batching, shuffling, and providing data in a way that is ready to be fed into your model for training or evaluation.\n",
    "- DataLoader for the training set, where batch_size=8 means each batch will contain 8 data samples, and shuffle=True ensures the data is shuffled before being loaded into batches\n",
    "- Wrapping the DataLoader in iter() converts it into an iterator, allowing you to use it in a for loop for batch-wise processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tE66Y0QfTuI8"
   },
   "outputs": [],
   "source": [
    "train_loader = iter(DataLoader(processed_data[:eighty_split], batch_size = 8, shuffle = True))\n",
    "val_loader = iter(DataLoader(processed_data[eighty_split:ninety_split], batch_size = 8, shuffle = True))\n",
    "test_loader = iter(DataLoader(processed_data[ninety_split:], batch_size = 8, shuffle = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbQIX3ewWfi-",
    "outputId": "e917fa31-0113-497b-be50-d61761c8cbb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5120])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(train_loader)\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbS7Tj4e3hDJ"
   },
   "source": [
    "a common technique in sequence-based tasks like language modeling. It splits the sequence into two parts: the input sequence (seq) and the target labels (labels), where the model is trained to predict the next token in the sequence.\n",
    "- example: This refers to a sequence of tokens, such as a sentence or document, represented as a tensor or array. Each element is usually a numerical representation of a token (e.g., a word or character).\n",
    "- [:, :-1]: This slices the sequence from the start (index 0) to the second-to-last element. In the context of language modeling, this is the input sequence (seq) that the model will use to predict the next token.\n",
    "- [:, 1:]: This slices the sequence starting from the second element (index 1) to the end. This is the target sequence (labels), which is used as the ground truth for training. The model tries to predict the token at index i+1 given the token at index i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V57wKYxVXAno",
    "outputId": "35bef15a-bf52-418e-a153-cd973cfc1eeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5119])\n",
      "tensor([ 32, 108, 101,  97, 115, 116,  32, 116, 119, 111,  32, 118, 101, 114,\n",
      "        116])\n",
      "torch.Size([8, 5119])\n",
      "tensor([108, 101,  97, 115, 116,  32, 116, 119, 111,  32, 118, 101, 114, 116,\n",
      "        105])\n"
     ]
    }
   ],
   "source": [
    "seq, labels = example[:, :-1], example[:, 1:]\n",
    "print(seq.shape)\n",
    "print(seq[0][:15])\n",
    "print(labels.shape)\n",
    "print(labels[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-rtF7JwXBSC",
    "outputId": "efd19586-176e-4f1a-9506-4ba14fce5f9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.chunk(10, dim=-1)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QljZgD84fUW"
   },
   "source": [
    "The code you provided iterates through segments of sequences (seq) and their corresponding labels (labels), each chunked into parts of size 10 (defined by chunk(10, dim=-1)), and prints the decoded text of the second token from each segment\n",
    "- chunk(10, dim=-1) splits the seq tensor into chunks of size 10 along the last dimension. The -1 refers to the last axis, so if seq is a 2D tensor of shape (batch_size, sequence_length), this operation splits the sequence into smaller subsequences of length 10.\n",
    "- zip pairs each chunk of seq with the corresponding chunk of labels. This creates an iterable where each element is a tuple containing a seq_segment and a labels_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJJa5wMacC_b",
    "outputId": "93235037-67a5-4b2d-f2f2-44d449dcc2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " carried out a literature search in order to identify known objects associated with the infrared point source centroids . \n",
      " table  [ targets ] lists the targeted point sources from this study , corresponding labels from @xcite and @xcite , and additional information from the literature . \n",
      " all point sources are identified as sites of recent or ongoing star formation , and are typically young protostars or young stellar clusters . a number of the point sources exhibit interesting spectral features that are w \n",
      " *********** \n",
      "\n",
      "orth a closer look . \n",
      " we describe these point sources in detail below . \n",
      " unresolved emission from two bright star clusters in n66 exhibit silicate emission features in their spectra : ngc  346 ( ps9 ) and n66b ( ps6 ) ; see figure  [ starclusters ] . \n",
      " both of these point sources are bright h@xmath9  sources @xcite , contain ` blue ' stars @xcite , and have been modeled as @xmath0  3  myr old with _ hubble _ color - magnitude diagrams @xcite . \n",
      " this age is consistent with the presence of remnant dust fro \n",
      " *********** \n",
      "\n",
      "m the natal cloud surrounding the cluster .    ) . \n",
      " both show pronounced silicate emission features . [ starclusters ] ]    the origin of the silicate emission is most likely an optically thin layer of intracluster and/or enveloping dust that has been heated to @xmath0  200  k by the stars in the central clusters . \n",
      " the presence of silicate emission suggests a relatively low optical depth ; this is supported by optical data of n66 ( see , for example , the high - resolution _ \n",
      " hubble _ data presented in  \n",
      " *********** \n",
      "\n",
      "* ? ? ? \n",
      " * ) , where the stars in the centers of the clusters are clearly visible through the intervening dust , and supported by the measured extinction by dust @xcite . \n",
      " this geometry is similar to clumpy model geometries for young star clusters presented in @xcite , where silicate emission / absorption was found to be dependent on the line - of - sight dust geometry . \n",
      " neither cluster is resolved in the spectral slit , and we therefore did not employ the irs ll module ( 15 @xmath19 @xmath11 @xmath19 3 \n",
      " *********** \n",
      "\n",
      "7 ) to measure the 17  silicate emission as in @xcite and @xcite because the flux mis - match between the sl and ll _ spitzer_/irs modules is very substantial and would bias the silicate dust temperature measurement . \n",
      " the presence of an o5.5v star in n66b and an o9v star in ngc  346 means that the point source extractions show a little [ s iv ] emission in n66b , none in ngc  346 , and [ neii ] emission in both . \n",
      " the [ siv ] emission in n66b is likely due to the o5.5v star . \n",
      " see @xcite , figure 7 for  \n",
      " *********** \n",
      "\n",
      "a map of the positions of the known o stars across n66 , many of which appear off of the stellar cluster positions . \n",
      " silicates in emission associated with young star clusters have not been observed regularly before . in ngc  3603 , \n",
      " pointings on and near the central star cluster show silicate emission @xcite . \n",
      " there is one pointing in 30  doradus in the lmc ( source b ; * ? ? ? \n",
      " * and also found in lebouteiller et al@xmath23 2008 ) near r136 that exhibits silicate emission , but this source has been s \n",
      " *********** \n",
      "\n",
      "pectroscopically identified as an m - type supergiant star @xcite . while there are numerous detections of silicates in emission among protoplanetary systems ( @xmath24 * ? ? ? \n",
      " * ; * ? ? ? \n",
      " * ) and evolved stars such as asymptotic giant branch stars ( agbs , both galactically and extragalactically ; see * ? ? ? \n",
      " * ; * ? ? ? \n",
      " * ) , there are relatively few young stellar clusters that show silicate emission . \n",
      " @xcite show that a diffuse silicate population is likely across the orion nebula . \n",
      " compared  \n",
      " *********** \n",
      "\n",
      "with those observations , the silicate emission observed in ngc  3603 , ngc  346 , and n66b is distinct in that it is clearly associated with the stellar clusters and not visibly dispersed across the region . \n",
      " it seems likely that the strong silicate emission associated with the star clusters in n66 and ngc  3603 is tied to a relatively short period of time in the early evolution of star clusters and will only last a short period of time ; that these clusters are definitively young ( e.g. * ? ? ? \n",
      " * ) and \n",
      " *********** \n",
      "\n",
      " therefore contain no agb stars excludes the possibility of silicate - rich winds from post - main sequence stars contributing to the observed silicate emission . \n",
      " there is a third spectrum exhibiting a silicate emission feature : ps8 . in this instance , the silicate feature is not as pronounced as for the star clusters discussed above , though it has pronounced pah features as well ( see figure  [ spec1 ] in the appendix ) \n",
      " . searching by position , we found a be star at those coordinates , cl * ngc  34 \n",
      " *********** \n",
      "\n",
      "6 kwbbe  200 . \n",
      " @xcite fit a uv - to-8  spectral energy distribution ( sed ) with a b - star template and a t  @xmath0  800  k blackbody , observed p  cygni profiles on a number of optical spectral lines , roughly determined a luminosity of 10@xmath25  l@xmath1 , and concluded that this source is a b[e ] supergiant . \n",
      " evidence against it being a herbig be system is that no inverse p  cygni profiles associated with infall were observed , and that the derived luminosity is on the high end for herbig be sta \n",
      " *********** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_segment, labels_segment in zip(seq.chunk(10, dim = -1), labels.chunk(10, dim = -1)):\n",
    "    print(decode_text(seq_segment[1]), \"\\n *********** \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajG2dKuutCVr"
   },
   "source": [
    "### Example Training Loop\n",
    "\n",
    "Model Description:\n",
    "- An embedding layer maps input tokens to a 16-dimensional space.\n",
    "- Hidden layers apply transformations with ReLU activation.\n",
    "- The output layer predicts the next token in the sequence.\n",
    "\n",
    "Training Loop:\n",
    "- Each batch is divided into 10 smaller chunks to fit memory constraints and optimize GPU utilization.\n",
    "- Backpropagation occurs for each chunk separately.\n",
    "\n",
    "Validation and Testing\n",
    "- Validation is conducted after every 50 iterations to monitor model performance without training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qzW_dKl4-5b",
    "outputId": "b939ea39-68aa-4eec-a3cf-2529eb992dba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(128, 16)\n",
       "  (1): Linear(in_features=16, out_features=150, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=150, out_features=150, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=150, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Embedding(128,16), # (vocab_size, embedding_dim) # Embedding layer\n",
    "    nn.Linear(16, 150),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150,150),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150, 128), # (params, vocab_size) # Output matches embedding size\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcL-yF2Fe4sj",
    "outputId": "3407aabd-3cc5-4637-bf34-87733a155f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.814817905426025\n",
      "3.471447801589966\n",
      "3.1637600898742675\n",
      "2.9949301719665526\n",
      "3.086688208580017\n",
      "2.7922783136367797\n",
      "2.8021355867385864\n",
      "2.7291361331939696\n",
      "2.9221490621566772\n",
      "2.6424169540405273\n",
      "2.6004881143569945\n",
      "VALIDATION LOSS 2.771096420288086\n",
      "2.5817805528640747\n",
      "2.5987738132476808\n",
      "2.6307487726211547\n",
      "2.694489097595215\n",
      "2.5678778648376466\n",
      "2.4874101638793946\n",
      "2.5722203254699707\n",
      "2.5212898015975953\n",
      "2.7539854288101195\n",
      "2.4800504207611085\n",
      "VALIDATION LOSS 2.565450644493103\n",
      "2.5101852416992188\n",
      "2.6612972974777223\n",
      "2.4917702674865723\n",
      "2.5158987998962403\n",
      "2.450358510017395\n",
      "2.4149893045425417\n",
      "2.460058665275574\n",
      "2.66586594581604\n",
      "2.5224470138549804\n",
      "2.465202474594116\n",
      "VALIDATION LOSS 2.730003571510315\n",
      "2.4767351388931274\n",
      "2.5565998792648315\n",
      "2.5127267360687258\n",
      "2.4015066385269166\n",
      "2.3902504444122314\n",
      "2.4590445041656492\n",
      "2.424598789215088\n",
      "2.4134016752243044\n",
      "2.390960693359375\n",
      "2.584371018409729\n",
      "VALIDATION LOSS 2.51204776763916\n",
      "2.6035442113876344\n",
      "2.417050862312317\n",
      "2.4082767963409424\n",
      "2.582948160171509\n",
      "2.520162510871887\n",
      "2.4698104858398438\n",
      "2.4466190338134766\n",
      "2.4519865036010744\n",
      "2.470808434486389\n",
      "2.537000608444214\n",
      "VALIDATION LOSS 2.445270299911499\n",
      "2.6722689628601075\n",
      "2.375909852981567\n",
      "2.4066864252090454\n",
      "2.5000575304031374\n",
      "2.5424475193023683\n",
      "2.4264968156814577\n",
      "2.4076618432998655\n",
      "2.514090323448181\n",
      "2.381501626968384\n"
     ]
    }
   ],
   "source": [
    "segments = 10\n",
    "for i in range(300):\n",
    "\n",
    "    data = next(train_loader) # (batch_size, sequence_length) # (8, 5120)\n",
    "    seq, labels = data[:, :-1], data[:, 1:]\n",
    "    train_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for seq_segment, labels_segment in zip(seq.chunk(segments, dim = -1), labels.chunk(segments, dim = -1)): # ten passes of (8, 512)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq_segment)\n",
    "        y_pred = y_pred.transpose(2,1) # Match shapes for CrossEntropyLoss# Match shapes for CrossEntropyLoss\n",
    "        loss = loss_fn(y_pred, labels_segment)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print (train_loss / segments)\n",
    "\n",
    "    if i > 0 and i % 50 == 0:\n",
    "        val_data = next(val_loader)\n",
    "        seq, labels = val_data[:, :-1], val_data[:, 1:]\n",
    "        eval_loss = 0.\n",
    "        model.eval()\n",
    "        for seq_segment, labels_segment in zip(seq.chunk(segments, dim = -1), labels.chunk(segments, dim = -1)): # ten passes of (8, 512)\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(seq_segment)\n",
    "                y_pred = y_pred.transpose(2,1)\n",
    "                loss = loss_fn(y_pred, labels_segment)\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        print (\"VALIDATION LOSS\", (eval_loss / segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgTeXjP7zf_H",
    "outputId": "f53dbfb5-be6e-445f-d8ea-3af67d97ac8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS 2.447762060165405\n"
     ]
    }
   ],
   "source": [
    "test_data = next(test_loader)\n",
    "seq, labels = test_data[:, :-1], test_data[:, 1:]\n",
    "test_loss = 0.\n",
    "model.eval()\n",
    "for seq_segment, labels_segment in zip(seq.chunk(segments, dim = -1), labels.chunk(segments, dim = -1)): # ten passes of (8, 512)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(seq_segment)\n",
    "        y_pred = y_pred.transpose(2,1)\n",
    "        loss = loss_fn(y_pred, labels_segment)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print (\"TEST LOSS\", (test_loss / segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLzo5suTtCVs"
   },
   "source": [
    "### Key Takeaways:\n",
    "\n",
    "- Efficient Data Processing: Splitting into chunks ensures GPU memory is optimally utilized.\n",
    "- Iterative Approach: Training loop processes small chunks sequentially to minimize memory overhead.\n",
    "- Dynamic Testing:Validation and testing steps ensure the model generalizes to unseen data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
