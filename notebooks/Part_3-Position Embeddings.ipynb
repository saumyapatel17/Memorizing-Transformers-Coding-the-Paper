{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWwX_X0l4Q_S"
      },
      "source": [
        "# Adding in Position Embeddings\n",
        "\n",
        "This code is part of a Transformer-based model (like T5) and deals with position embeddings. Position embeddings are used to inject information about the position of tokens (words) in a sequence so that the model can take the order of tokens into account. The goal of this code is to add relative position embeddings to a Transformer’s attention mechanism, which helps the model understand how far apart tokens are from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILlnkDafi1r8"
      },
      "source": [
        "From Memorizing Transformers paper:\n",
        "\n",
        "    \"Position bias. For dense attention within the local context, we use the T5 relative position bias (Raffel\n",
        "    et al., 2020). As noted by Dai et al. (2019), adding a global position encoding to each token does not\n",
        "    work well when processing long documents. We don’t use a position bias for the retrieved memories.\n",
        "    Experiments on the PG19 dataset (Sun et al., 2021) have shown that relative position does not appear\n",
        "    to matter at long range, and the T5 relative bias puts all long-range tokens in the same bucket anyway.\"\n",
        "\n",
        "From T5 paper:\n",
        "\n",
        "    \"Since self-attention is order-independent (i.e. it is an operation on sets), it is common\n",
        "    to provide an explicit position signal to the Transformer. While the original Transformer\n",
        "    used a sinusoidal position signal or learned position embeddings, it has recently become\n",
        "    more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\n",
        "    Instead of using a fixed embedding for each position, relative position embeddings produce\n",
        "    a different learned embedding according to the offset between the “key” and “query” being\n",
        "    compared in the self-attention mechanism. We use a simplified form of position embeddings\n",
        "    where each “embedding” is simply a scalar that is added to the corresponding logit used\n",
        "    for computing the attention weights. For efficiency, we also share the position embedding\n",
        "    parameters across all layers in our model, though within a given layer each attention head\n",
        "    uses a different learned position embedding. Typically, a fixed number of embeddings are\n",
        "    learned, each corresponding to a range of possible key-query offsets. In this work, we use 32\n",
        "    embeddings for all of our models with ranges that increase in size logarithmically up to an\n",
        "    offset of 128 beyond which we assign all relative positions to the same embedding. Note\n",
        "    that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers\n",
        "    can build a sensitivity to larger offsets by combining local information from previous layers.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adAdFNxyjlNI"
      },
      "outputs": [],
      "source": [
        "# RELATIVE POSITION MATRIX\n",
        "# tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13],\n",
        "#         [ -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12],\n",
        "#         [ -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11],\n",
        "#         [ -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10],\n",
        "#         [ -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9],\n",
        "#         [ -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8],\n",
        "#         [ -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7],\n",
        "#         [ -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6],\n",
        "#         [ -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5],\n",
        "#         [ -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4],\n",
        "#         [-10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3],\n",
        "#         [-11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2],\n",
        "#         [-12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1],\n",
        "#         [-13, -12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu22AEE_qEoF"
      },
      "source": [
        "### Idea\n",
        "\n",
        "- Positional embeddings are added to the QK embeddings during attention\n",
        "- Relative position embeddings identify, for each input example, how far away all the other tokens are from a specific token of interest\n",
        "- Instead of giving each token a relative position index of n that is n positions away from our token of interest, T5 relative position \"buckets\" some tokens into the same index\n",
        "- First we create this set of indices. then the indices are matched to an embedding layer of weight values. These values are then added to the QK embeddings during attention. The positional embeddings are trained with the network.\n",
        "\n",
        "### Recipe\n",
        "\n",
        "- Construct a relative position matrix\n",
        "- For offsets larger than what we want, start to spread offset values logarithmically into a finite amount of buckets. (Past a certian max value (128) we'll just map everything to one value)\n",
        "- Initialize embedding weights that we will assign offset values to\n",
        "- Now the relative position matrix is mapped to these weights\n",
        "- This matrix gets added to our attention when we perform self-attention. Our self-attention now incorporates as a piece of information the relative positions between tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6umaDkN4Q_a"
      },
      "source": [
        "### Imports and Setup\n",
        "\n",
        "- numpy: Used for numerical operations (not directly used in this snippet but often used for array manipulations).\n",
        "- torch: The deep learning framework, used here to create tensors and perform operations.\n",
        "- torch.nn: Contains layers, such as nn.Embedding, which is used for the position embeddings.\n",
        "- einsum: A special function for complex tensor operations, not used in this block directly.\n",
        "- torch.nn.functional: Contains functions like activation functions, loss functions, etc., though not used here.\n",
        "- math: The math library for functions like log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jL57MdNEjlPP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I9xr3in4Q_b"
      },
      "source": [
        "### Set Parameters for Position Embedding\n",
        "\n",
        "- num_buckets: Defines how many \"buckets\" or categories you will have for the relative positions between tokens. For example, positions from 1 to 5 might be grouped into one bucket, and 6 to 10 into another.\n",
        "- max_distance: The longest distance between two tokens in the context the model will consider.\n",
        "- sequence_length and max_context_length: Define how long your input and key sequences are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KYhQ7Xwqg6Fu"
      },
      "outputs": [],
      "source": [
        "num_buckets = 6 # the total number of index buckets we'll use\n",
        "max_distance = 20 # maximum sequence length\n",
        "\n",
        "sequence_length = 14 # query length / input sequence length\n",
        "max_context_length = 14 # key length: can be equal to sequence_length or greater if recurrence/memory is concatenated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro77lr4y4Q_d"
      },
      "source": [
        "### Creating the Relative Position Matrix\n",
        "\n",
        "- q_pos: Creates a sequence of numbers from 0 to sequence_length - 1, representing the positions of tokens in the query.\n",
        "- k_pos: Similar to q_pos, but for the key sequence.\n",
        "- rel_pos: This computes the relative positions between the query and key tokens by subtracting the query position from the - key position. This matrix represents the distance between tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zByscJRIg7BW",
        "outputId": "12d028ff-4f40-4a7d-accc-0367757ee4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])\n",
            "torch.Size([14])\n",
            "tensor([[ 0],\n",
            "        [ 1],\n",
            "        [ 2],\n",
            "        [ 3],\n",
            "        [ 4],\n",
            "        [ 5],\n",
            "        [ 6],\n",
            "        [ 7],\n",
            "        [ 8],\n",
            "        [ 9],\n",
            "        [10],\n",
            "        [11],\n",
            "        [12],\n",
            "        [13]])\n",
            "torch.Size([14, 1])\n",
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])\n",
            "torch.Size([14])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13],\n",
              "        [ -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12],\n",
              "        [ -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11],\n",
              "        [ -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10],\n",
              "        [ -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9],\n",
              "        [ -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8],\n",
              "        [ -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7],\n",
              "        [ -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6],\n",
              "        [ -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5],\n",
              "        [ -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4],\n",
              "        [-10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3],\n",
              "        [-11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2],\n",
              "        [-12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1],\n",
              "        [-13, -12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "q_pos = torch.arange(sequence_length, dtype=torch.long)\n",
        "print(q_pos)\n",
        "print(q_pos.shape)\n",
        "\n",
        "q_pos = q_pos.reshape(q_pos.shape[0], 1)\n",
        "print(q_pos)\n",
        "print(q_pos.shape)\n",
        "\n",
        "k_pos = torch.arange(max_context_length, dtype=torch.long)\n",
        "print(k_pos)\n",
        "print(k_pos.shape)\n",
        "\n",
        "rel_pos = k_pos - q_pos\n",
        "rel_pos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXNh_fz_4Q_f"
      },
      "source": [
        "### Handling Negative and Large Values in Positions\n",
        "\n",
        "- n = -rel_pos: Makes all the relative positions negative because the model is concerned with how far tokens are to the right (positive) or to the left (negative).\n",
        "- torch.max(n, torch.zeros_like(n)): Ensures that any negative values (which can happen if a token is far to the left) are set to zero, because relative position can't be negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFhYyBMzg7FV",
        "outputId": "84d417c7-ccb4-41f0-bca2-569b021964ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11, -12, -13],\n",
              "        [  1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11, -12],\n",
              "        [  2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11],\n",
              "        [  3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10],\n",
              "        [  4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9],\n",
              "        [  5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8],\n",
              "        [  6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7],\n",
              "        [  7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6],\n",
              "        [  8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5],\n",
              "        [  9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3,  -4],\n",
              "        [ 10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2,  -3],\n",
              "        [ 11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1,  -2],\n",
              "        [ 12,  11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0,  -1],\n",
              "        [ 13,  12,  11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "n = -rel_pos\n",
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U_OD9UYg7HW",
        "outputId": "1c7fc690-42e0-4ab1-a2b3-7d9880f3e9d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0,  0],\n",
              "        [ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0,  0],\n",
              "        [10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0,  0],\n",
              "        [11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0,  0],\n",
              "        [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  0],\n",
              "        [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "n = torch.max(n, torch.zeros_like(n))\n",
        "n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XBCB-rB4Q_g"
      },
      "source": [
        "### Handling Small and Large Offsets\n",
        "\n",
        "- max_exact: Half of the buckets will deal with exact increments, like +1, +2, etc.\n",
        "- is_small: A boolean mask identifying which relative positions are small enough to be assigned exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt3zVGBRg7JQ",
        "outputId": "7ecbfc37-364d-4a1c-ccee-64ad0a4295fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# half of the buckets are for exact increments in positions\n",
        "max_exact = num_buckets // 2\n",
        "max_exact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t62LeaWgg7LZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39abe631-ddd8-4cbd-8871-8d7d6a8444ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False,  True,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False,  True,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False, False,  True,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False, False, False,\n",
              "          True,  True,  True,  True],\n",
              "        [False, False, False, False, False, False, False, False, False, False,\n",
              "         False,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "is_small = n < max_exact\n",
        "is_small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7hE2zQHsMQh"
      },
      "source": [
        "The other half of the buckets are for logarithmically bigger bins in positions up to max_distance.\n",
        "\n",
        "So, we map a positional embeddings up to a number k exactly (offset by 1, offset by 2, offset by 3...) but at a certain point we have a longer sequence than positional embedding \"buckets\" (like bins), so we map them logarithmically to spread out over our fixed number of buckets: e.g. [1,2,3,4,5,5,6,6,7,7,7,8,8,8,8,]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ5CLwsp4Q_h"
      },
      "source": [
        "### Handling Large Offsets Logarithmically\n",
        "\n",
        "For positions that are large, the model uses logarithmic scaling. This means that instead of directly mapping each position to a unique bucket, larger distances will be grouped together, and each group corresponds to a bucket.\n",
        "\n",
        "The formula essentially takes the log of the position, scales it, and\n",
        "\n",
        "The positions that are far apart (large n values) are handled differently. Instead of directly assigning them a unique bucket, we use a logarithmic scale to group them into a smaller number of buckets.\n",
        "- torch.log(n.float() / max_exact): Takes the logarithm of the relative position divided by the maximum exact position.\n",
        "- math.log(max_distance / max_exact): Normalizes the logarithmic scale to fit within the number of buckets.\n",
        "- torch.min(val_if_large, ...): Ensures that the values don't exceed the maximum bucket value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B5_F7vJtg7Nn"
      },
      "outputs": [],
      "source": [
        "# Not sure why this function looks the way it does! But it maps indices logarithmically over a fixed number of buckets.\n",
        "val_if_large = max_exact + \\\n",
        "  (\n",
        "    torch.log(n.float() / max_exact)  # log of matrix divided by scalar\n",
        "    / math.log(max_distance / max_exact) * (num_buckets - max_exact) # scalar\n",
        "    ).long() # convert float to int\n",
        "\n",
        "val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBRfkYuPsJCb",
        "outputId": "2512b762-0033-43bf-a79f-3295e5e2f05f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,   -inf,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588, 1.2627,\n",
              "           -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000, 2.3588,\n",
              "         1.2627,   -inf,   -inf,   -inf,   -inf],\n",
              "        [5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549, 3.0000,\n",
              "         2.3588, 1.2627,   -inf,   -inf,   -inf],\n",
              "        [5.1922, 5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078, 3.4549,\n",
              "         3.0000, 2.3588, 1.2627,   -inf,   -inf],\n",
              "        [5.3188, 5.1922, 5.0546, 4.9039, 4.7373, 4.5510, 4.3399, 4.0961, 3.8078,\n",
              "         3.4549, 3.0000, 2.3588, 1.2627,   -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "val_if_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlndkk1zsJEj",
        "outputId": "97a81cdc-9e67-4df9-8b2b-1eec4a4b6057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   3,                    3,                    2,\n",
              "                            1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   3,                    3,                    3,\n",
              "                            2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    3,                    3,\n",
              "                            3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    3,\n",
              "                            3,                    3,                    2,\n",
              "                            1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    4,\n",
              "                            3,                    3,                    3,\n",
              "                            2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    4,\n",
              "                            4,                    3,                    3,\n",
              "                            3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   4,                    4,                    4,\n",
              "                            4,                    4,                    3,\n",
              "                            3,                    3,                    2,\n",
              "                            1, -9223372036854775808, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   5,                    4,                    4,\n",
              "                            4,                    4,                    4,\n",
              "                            3,                    3,                    3,\n",
              "                            2,                    1, -9223372036854775808,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   5,                    5,                    4,\n",
              "                            4,                    4,                    4,\n",
              "                            4,                    3,                    3,\n",
              "                            3,                    2,                    1,\n",
              "         -9223372036854775808, -9223372036854775808],\n",
              "        [                   5,                    5,                    5,\n",
              "                            4,                    4,                    4,\n",
              "                            4,                    4,                    3,\n",
              "                            3,                    3,                    2,\n",
              "                            1, -9223372036854775808]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "val_if_large = val_if_large.long()\n",
        "val_if_large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTpDAsSQ4Q_i"
      },
      "source": [
        "### Creating Position Bucket Indices\n",
        "\n",
        "This line decides whether to use the exact bucket (n) or the logarithmically scaled bucket (val_if_large), based on whether the position is small or large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVwInlBQsJG0",
        "outputId": "aa166f7d-13ab-47aa-a968-567e38f32b30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0],\n",
              "        [4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0, 0],\n",
              "        [5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0, 0],\n",
              "        [5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0, 0],\n",
              "        [5, 5, 5, 4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "position_bucket_indices = torch.where(is_small, n, val_if_large)\n",
        "position_bucket_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OFa2gF4Q_i"
      },
      "source": [
        "### Initializing the Position Embedding Layer\n",
        "\n",
        "This line initializes a learnable embedding layer for the position biases. The layer will learn embeddings for each bucket (relative position), which will be used in the attention calculation. The number of buckets is num_buckets, and the number of attention heads is heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdC741IDg7Pt",
        "outputId": "a1d8ecc0-0dd8-4848-d488-66a5b7c75c5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(6, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "heads = 4\n",
        "relative_position_bias = nn.Embedding(num_buckets, heads)\n",
        "relative_position_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf9eo0CBZcZZ",
        "outputId": "04f29a60-1257-461b-cbc5-d9f7a65532f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.3185,  0.7457, -0.3274, -0.2323],\n",
              "        [-0.7818, -0.3635, -1.0704,  0.7617],\n",
              "        [-0.0420,  0.5257,  0.8047,  0.7205],\n",
              "        [-1.0042, -0.6981, -1.2029,  1.3093],\n",
              "        [-0.5956, -0.1140,  0.5519,  0.1065],\n",
              "        [ 0.9892, -0.0077,  0.5691, -0.5393]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "relative_position_bias.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy8wF8ry4Q_j"
      },
      "source": [
        "Here, the relative_position_bias embedding layer is used to map the position_bucket_indices (the indices for each relative position) into embeddings. This converts the indices into vector representations that the model can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0eXx36QtnZC",
        "outputId": "6ddbf79d-cdab-4752-8f81-445987775773"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14, 14, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "relative_position_values = relative_position_bias(position_bucket_indices)\n",
        "relative_position_values.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIQjAAE4Q_k"
      },
      "source": [
        "- transpose(0, 2): This swaps the sequence and context dimensions so that the shape aligns for attention computation.\n",
        "- unsqueeze(0): Adds a batch dimension (since models typically expect a batch of data, even if there’s only one example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pahYMG0Av1pa",
        "outputId": "c16ed627-981d-4117-9824-4a027c9b54b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 14, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Need to reshape from (sequence, context, heads) -> (batch, heads, sequence, context)\n",
        "relative_position_values = relative_position_values.transpose(0,2).unsqueeze(0)\n",
        "relative_position_values.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relative_position_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYBC88pQ-jaP",
        "outputId": "36d0743e-b544-4bb9-a00b-de3d6a9936d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.3185, -0.7818, -0.0420, -1.0042, -1.0042, -1.0042, -0.5956,\n",
              "           -0.5956, -0.5956, -0.5956, -0.5956,  0.9892,  0.9892,  0.9892],\n",
              "          [-0.3185, -0.3185, -0.7818, -0.0420, -1.0042, -1.0042, -1.0042,\n",
              "           -0.5956, -0.5956, -0.5956, -0.5956, -0.5956,  0.9892,  0.9892],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.7818, -0.0420, -1.0042, -1.0042,\n",
              "           -1.0042, -0.5956, -0.5956, -0.5956, -0.5956, -0.5956,  0.9892],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.7818, -0.0420, -1.0042,\n",
              "           -1.0042, -1.0042, -0.5956, -0.5956, -0.5956, -0.5956, -0.5956],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.7818, -0.0420,\n",
              "           -1.0042, -1.0042, -1.0042, -0.5956, -0.5956, -0.5956, -0.5956],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.7818,\n",
              "           -0.0420, -1.0042, -1.0042, -1.0042, -0.5956, -0.5956, -0.5956],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.7818, -0.0420, -1.0042, -1.0042, -1.0042, -0.5956, -0.5956],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.3185, -0.7818, -0.0420, -1.0042, -1.0042, -1.0042, -0.5956],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.3185, -0.3185, -0.7818, -0.0420, -1.0042, -1.0042, -1.0042],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.3185, -0.3185, -0.3185, -0.7818, -0.0420, -1.0042, -1.0042],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.3185, -0.3185, -0.3185, -0.3185, -0.7818, -0.0420, -1.0042],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.7818, -0.0420],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.7818],\n",
              "          [-0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185,\n",
              "           -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185, -0.3185]],\n",
              "\n",
              "         [[ 0.7457, -0.3635,  0.5257, -0.6981, -0.6981, -0.6981, -0.1140,\n",
              "           -0.1140, -0.1140, -0.1140, -0.1140, -0.0077, -0.0077, -0.0077],\n",
              "          [ 0.7457,  0.7457, -0.3635,  0.5257, -0.6981, -0.6981, -0.6981,\n",
              "           -0.1140, -0.1140, -0.1140, -0.1140, -0.1140, -0.0077, -0.0077],\n",
              "          [ 0.7457,  0.7457,  0.7457, -0.3635,  0.5257, -0.6981, -0.6981,\n",
              "           -0.6981, -0.1140, -0.1140, -0.1140, -0.1140, -0.1140, -0.0077],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457, -0.3635,  0.5257, -0.6981,\n",
              "           -0.6981, -0.6981, -0.1140, -0.1140, -0.1140, -0.1140, -0.1140],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457, -0.3635,  0.5257,\n",
              "           -0.6981, -0.6981, -0.6981, -0.1140, -0.1140, -0.1140, -0.1140],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457, -0.3635,\n",
              "            0.5257, -0.6981, -0.6981, -0.6981, -0.1140, -0.1140, -0.1140],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "           -0.3635,  0.5257, -0.6981, -0.6981, -0.6981, -0.1140, -0.1140],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "            0.7457, -0.3635,  0.5257, -0.6981, -0.6981, -0.6981, -0.1140],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "            0.7457,  0.7457, -0.3635,  0.5257, -0.6981, -0.6981, -0.6981],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "            0.7457,  0.7457,  0.7457, -0.3635,  0.5257, -0.6981, -0.6981],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "            0.7457,  0.7457,  0.7457,  0.7457, -0.3635,  0.5257, -0.6981],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "            0.7457,  0.7457,  0.7457,  0.7457,  0.7457, -0.3635,  0.5257],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "            0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457, -0.3635],\n",
              "          [ 0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,\n",
              "            0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457,  0.7457]],\n",
              "\n",
              "         [[-0.3274, -1.0704,  0.8047, -1.2029, -1.2029, -1.2029,  0.5519,\n",
              "            0.5519,  0.5519,  0.5519,  0.5519,  0.5691,  0.5691,  0.5691],\n",
              "          [-0.3274, -0.3274, -1.0704,  0.8047, -1.2029, -1.2029, -1.2029,\n",
              "            0.5519,  0.5519,  0.5519,  0.5519,  0.5519,  0.5691,  0.5691],\n",
              "          [-0.3274, -0.3274, -0.3274, -1.0704,  0.8047, -1.2029, -1.2029,\n",
              "           -1.2029,  0.5519,  0.5519,  0.5519,  0.5519,  0.5519,  0.5691],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -1.0704,  0.8047, -1.2029,\n",
              "           -1.2029, -1.2029,  0.5519,  0.5519,  0.5519,  0.5519,  0.5519],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -1.0704,  0.8047,\n",
              "           -1.2029, -1.2029, -1.2029,  0.5519,  0.5519,  0.5519,  0.5519],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -1.0704,\n",
              "            0.8047, -1.2029, -1.2029, -1.2029,  0.5519,  0.5519,  0.5519],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -1.0704,  0.8047, -1.2029, -1.2029, -1.2029,  0.5519,  0.5519],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -0.3274, -1.0704,  0.8047, -1.2029, -1.2029, -1.2029,  0.5519],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -0.3274, -0.3274, -1.0704,  0.8047, -1.2029, -1.2029, -1.2029],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -0.3274, -0.3274, -0.3274, -1.0704,  0.8047, -1.2029, -1.2029],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -0.3274, -0.3274, -0.3274, -0.3274, -1.0704,  0.8047, -1.2029],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -1.0704,  0.8047],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -1.0704],\n",
              "          [-0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274,\n",
              "           -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274, -0.3274]],\n",
              "\n",
              "         [[-0.2323,  0.7617,  0.7205,  1.3093,  1.3093,  1.3093,  0.1065,\n",
              "            0.1065,  0.1065,  0.1065,  0.1065, -0.5393, -0.5393, -0.5393],\n",
              "          [-0.2323, -0.2323,  0.7617,  0.7205,  1.3093,  1.3093,  1.3093,\n",
              "            0.1065,  0.1065,  0.1065,  0.1065,  0.1065, -0.5393, -0.5393],\n",
              "          [-0.2323, -0.2323, -0.2323,  0.7617,  0.7205,  1.3093,  1.3093,\n",
              "            1.3093,  0.1065,  0.1065,  0.1065,  0.1065,  0.1065, -0.5393],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323,  0.7617,  0.7205,  1.3093,\n",
              "            1.3093,  1.3093,  0.1065,  0.1065,  0.1065,  0.1065,  0.1065],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323,  0.7617,  0.7205,\n",
              "            1.3093,  1.3093,  1.3093,  0.1065,  0.1065,  0.1065,  0.1065],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,  0.7617,\n",
              "            0.7205,  1.3093,  1.3093,  1.3093,  0.1065,  0.1065,  0.1065],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "            0.7617,  0.7205,  1.3093,  1.3093,  1.3093,  0.1065,  0.1065],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "           -0.2323,  0.7617,  0.7205,  1.3093,  1.3093,  1.3093,  0.1065],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "           -0.2323, -0.2323,  0.7617,  0.7205,  1.3093,  1.3093,  1.3093],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "           -0.2323, -0.2323, -0.2323,  0.7617,  0.7205,  1.3093,  1.3093],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "           -0.2323, -0.2323, -0.2323, -0.2323,  0.7617,  0.7205,  1.3093],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "           -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,  0.7617,  0.7205],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "           -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,  0.7617],\n",
              "          [-0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323,\n",
              "           -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323, -0.2323]]]],\n",
              "       grad_fn=<UnsqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0NL_7074Q_k"
      },
      "source": [
        "\n",
        "Creating the RelativePosition Class\n",
        "- This is the definition of a new class called RelativePosition, which inherits from nn.Module. This class is responsible for creating and managing relative position embeddings within the model.\n",
        "- The constructor initializes several parameters, including num_buckets, rp_max_distance, and heads.\n",
        "\n",
        "Computing Position Buckets in the forward Method\n",
        "- sequence_pos: Creates a tensor representing the positions of tokens in the query sequence.\n",
        "- context_pos: Similarly, represents the positions in the context (key) sequence.\n",
        "- rel_pos: The relative positions between the query and context sequences.\n",
        "\n",
        "Computing the Final Relative Position Embedding\n",
        "- rp_values: Retrieves the position embeddings based on the position_bucket_indices.\n",
        "- rp_values.transpose(0, 2): Adjusts the dimensions to match the attention mechanism's requirements.\n",
        "- rp_values.unsqueeze(0): Adds a batch dimension.\n",
        "- return rp_values * self.scale: The relative position values are scaled (by rp_scale) and returned."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **RelativePosition** embedding system is a way of encoding positional relationships between tokens (e.g., words) in a sequence for use in the attention mechanism of a transformer model. This is especially useful in cases where the model needs to handle varying sequence lengths and efficiently represent the distance between tokens without explicitly storing all pairwise positional distances.\n",
        "\n",
        "### Key Ideas Behind Relative Position Embeddings\n",
        "\n",
        "1. **Relative Positions**:\n",
        "   - Instead of assigning each position in a sequence an absolute value (e.g., 1, 2, 3, ...), relative position embeddings focus on the distance between tokens.\n",
        "   - For example, if you're at position `i`, the token at position `j` has a relative position of `j - i`.\n",
        "\n",
        "2. **Position Bucketing**:\n",
        "   - Large relative distances can make models inefficient. Instead of representing every possible relative distance, we **group relative distances into buckets**.\n",
        "   - Example: Distances `1-5` might go into bucket `0`, distances `6-10` into bucket `1`, and so on. For large distances, logarithmic scaling is used to group distances more coarsely (e.g., `20-40`, `40-80`).\n",
        "\n",
        "3. **Relative Position Embeddings**:\n",
        "   - Each bucket is assigned an embedding vector.\n",
        "   - These embeddings are added or applied as bias terms to the attention mechanism, enriching the model's understanding of how tokens relate to each other based on their distance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Code Explanation**\n",
        "\n",
        "#### Key Components:\n",
        "\n",
        "1. **Initialization**:\n",
        "   ```python\n",
        "   self.relative_attention_embedding = nn.Embedding(num_buckets, heads)\n",
        "   ```\n",
        "   - An embedding matrix of shape `(num_buckets, heads)` is created. Each bucket gets a unique embedding for each attention head.\n",
        "\n",
        "2. **Position Bucketing**:\n",
        "   ```python\n",
        "   def relative_position_bucket(self, relative_position_matrix):\n",
        "       n = -relative_position_matrix\n",
        "       n = torch.max(n, torch.zeros_like(n))\n",
        "       ...\n",
        "   ```\n",
        "   - Relative positions (`rel_pos`) are mapped into buckets. Small distances (`< max_exact`) are preserved exactly, while larger distances are grouped logarithmically.\n",
        "\n",
        "   Example:\n",
        "   - `rel_pos = [-2, -1, 0, 1, 2, 10, 50]`\n",
        "   - Buckets might map these to `[2, 1, 0, 1, 2, 5, 8]`.\n",
        "\n",
        "3. **Forward Pass**:\n",
        "   ```python\n",
        "   sequence_pos = torch.arange(sequence_length, dtype=torch.long)\n",
        "   rel_pos = context_rel_pos - sequence_rel_pos\n",
        "   ```\n",
        "   - A relative position matrix (`rel_pos`) is calculated for all token pairs in the sequence.\n",
        "\n",
        "   ```python\n",
        "   position_bucket_indices = self.relative_position_bucket(rel_pos)\n",
        "   rp_values = self.relative_attention_embedding(position_bucket_indices)\n",
        "   ```\n",
        "   - The relative positions are converted to bucket indices and replaced with their corresponding embeddings.\n",
        "\n",
        "4. **Output**:\n",
        "   ```python\n",
        "   rp_values = rp_values.transpose(0,2).unsqueeze(0)\n",
        "   return rp_values * self.scale\n",
        "   ```\n",
        "   - The embeddings are reshaped for use in the attention mechanism, with a scaling factor (`self.scale`) applied to adjust their magnitude.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "#### Input:\n",
        "- Sequence length = 4\n",
        "- Context length = 6\n",
        "- Relative positions:\n",
        "  ```\n",
        "  [[0, 1, 2, 3, 4, 5],\n",
        "   [-1, 0, 1, 2, 3, 4],\n",
        "   [-2, -1, 0, 1, 2, 3],\n",
        "   ...\n",
        "  ]\n",
        "  ```\n",
        "\n",
        "#### Bucketing:\n",
        "- Exact for small distances:\n",
        "  - `[-2, -1, 0, 1, 2]` → `[2, 1, 0, 1, 2]`\n",
        "- Logarithmic for large distances:\n",
        "  - `3 → 3`, `4 → 4`, `5 → 4` (log scale simplifies larger values).\n",
        "\n",
        "#### Attention Impact:\n",
        "The embeddings guide the model to:\n",
        "- Focus on nearby tokens more precisely for short contexts (e.g., neighboring words).\n",
        "- Handle distant tokens efficiently for long sequences (e.g., paragraph-level understanding).\n",
        "\n",
        "---\n",
        "\n",
        "### Why Use Relative Position Embeddings?\n",
        "\n",
        "1. **Efficiency for Long Sequences**:\n",
        "   - By using buckets and logarithmic scaling, memory requirements are reduced for large sequences.\n",
        "\n",
        "2. **Better Performance on Variable-Length Sequences**:\n",
        "   - Unlike absolute embeddings, relative embeddings generalize well when the sequence length varies during training and inference.\n",
        "\n",
        "3. **Context Awareness**:\n",
        "   - Tokens are enriched with relative positional context, improving model performance in tasks like language modeling and translation."
      ],
      "metadata": {
        "id": "9hHSTx2-_utv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W4zmUH7Etndk"
      },
      "outputs": [],
      "source": [
        "class RelativePosition(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      rp_scale,\n",
        "      num_buckets = 32,\n",
        "      rp_max_distance = 128,\n",
        "      heads = 8\n",
        "  ):\n",
        "      super().__init__()\n",
        "      self.scale = rp_scale\n",
        "      self.num_buckets = num_buckets\n",
        "      self.rp_max_distance = rp_max_distance\n",
        "      self.relative_attention_embedding = nn.Embedding(num_buckets, heads)\n",
        "\n",
        "  def relative_position_bucket(self, relative_position_matrix):\n",
        "      n = -relative_position_matrix\n",
        "      n = torch.max(n, torch.zeros_like(n))\n",
        "\n",
        "      max_exact = self.num_buckets // 2\n",
        "\n",
        "      is_small = n < max_exact\n",
        "      val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(self.rp_max_distance / max_exact) * (self.num_buckets - max_exact)).long()\n",
        "      val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, self.num_buckets - 1))\n",
        "\n",
        "      return torch.where(is_small, n, val_if_large)\n",
        "\n",
        "  def forward(self, sequence_length, max_context_length):\n",
        "\n",
        "      sequence_pos = torch.arange(sequence_length, dtype=torch.long)\n",
        "      context_pos = torch.arange(max_context_length, dtype=torch.long)\n",
        "      sequence_pos = sequence_pos.reshape(sequence_pos.shape[0], 1)\n",
        "      rel_pos = context_rel_pos - sequence_rel_pos\n",
        "\n",
        "      position_bucket_indices = self.relative_position_bucket(rel_pos)\n",
        "\n",
        "      rp_values = self.relative_attention_embedding(position_bucket_indices)\n",
        "      # Rearrange (sequence, context, heads) -> (1, heads, sequence, context)\n",
        "      rp_values = rp_values.transpose(0,2)\n",
        "      rp_values = rp_values.unsqueeze(0)\n",
        "      return rp_values * self.scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onvnn7WX4Q_n"
      },
      "source": [
        "This part of the code is responsible for creating relative position embeddings that are added to the query and key vectors in the attention mechanism.\n",
        "\n",
        "Position buckets are used to group relative positions into a fixed number of categories (buckets).\n",
        "\n",
        "Logarithmic scaling is applied to large distances to ensure the model can handle long-range dependencies efficiently.\n",
        "\n",
        "The RelativePosition class calculates and returns the relative position embeddings, which are then used in the attention mechanism of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgQDjiJYtnfZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}